{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Notebook specific deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install kagglehub seaborn scikit-learn imblearn safetensors jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "DATASET_PATH = Path(\"./dataset/pima-indians-diabetes-database\")\n",
    "\n",
    "if not DATASET_PATH.exists():\n",
    "    # Download latest version\n",
    "    path = kagglehub.dataset_download(\"uciml/pima-indians-diabetes-database\")\n",
    "    shutil.move(path, DATASET_PATH)\n",
    "else:\n",
    "    print(f\"{DATASET_PATH} exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Dataset Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "column_names = [\n",
    "    \"Pregnancies\",\n",
    "    \"Glucose\",\n",
    "    \"BloodPressure\",\n",
    "    \"SkinThickness\",\n",
    "    \"Insulin\",\n",
    "    \"BMI\",\n",
    "    \"DiabetesPedigreeFunction\",\n",
    "    \"Age\",\n",
    "    \"y\",\n",
    "]\n",
    "df_diabetes = pd.read_csv(DATASET_PATH / \"diabetes.csv\", header=0, names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diabetes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diabetes.hist(figsize=(9, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Target value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diabetes[\"y\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x=\"y\", data=df_diabetes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_diabetes['y'].value_counts().plot(kind='pie')\n",
    "plt.pie(df_diabetes[\"y\"].value_counts().tolist(), labels=[\"0\", \"1\"], autopct=\"%1.1f%%\")\n",
    "\n",
    "plt.title(\"Outcomes\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Diabetes % by age range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Age ranges\n",
    "age_bins = [20, 30, 40, 50, 60, 70, 80]  # Adjust the age ranges as needed\n",
    "age_labels = [\"20-29\", \"30-39\", \"40-49\", \"50-59\", \"60-69\", \"70+\"]\n",
    "\n",
    "# Use the pd.cut function to categorize ages into age ranges:\n",
    "df_diabetes[\"AgeRange\"] = pd.cut(\n",
    "    df_diabetes[\"Age\"], bins=age_bins, labels=age_labels, include_lowest=True\n",
    ")\n",
    "\n",
    "# Group the data by age range and calculate the percentage of diabetes cases (Diabetes=1):\n",
    "age_diabetes = df_diabetes.groupby(\"AgeRange\", observed=True)[\"y\"].mean() * 100\n",
    "\n",
    "# Plot the data:\n",
    "plt.bar(age_diabetes.index, age_diabetes)\n",
    "plt.xlabel(\"Age Range\")\n",
    "plt.ylabel(\"Percentage with Diabetes\")\n",
    "plt.title(\"Diabetes Percentage by Age Range\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### BMI vs. Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BMI ranges\n",
    "bmi_ranges = [0, 18.5, 24.9, 29.9, 34.9, 100]\n",
    "bmi_labels = [\"Underweight\", \"Normal\", \"Overweight\", \"Obese (I)\", \"Obese (II+)\"]\n",
    "\n",
    "# Use the pd.cut function to categorize ages into age ranges:\n",
    "df_diabetes[\"BMIRange\"] = pd.cut(\n",
    "    df_diabetes[\"BMI\"], bins=bmi_ranges, labels=bmi_labels, include_lowest=True\n",
    ")\n",
    "\n",
    "# Group the data by age range and calculate the percentage of diabetes cases (Diabetes=1):\n",
    "bmi_diabetes = df_diabetes.groupby(\"BMIRange\", observed=True)[\"y\"].mean() * 100\n",
    "\n",
    "# Plot the data:\n",
    "plt.bar(bmi_diabetes.index, bmi_diabetes)\n",
    "plt.xlabel(\"BMI Range\")\n",
    "plt.ylabel(\"Percentage with Diabetes\")\n",
    "plt.title(\"Diabetes Percentage by BMI Range\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Glucose vs Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two separate data series for diabetic and non-diabetic patients:\n",
    "glucose_diabetic = df_diabetes[df_diabetes[\"y\"] == 1][\"Glucose\"]\n",
    "\n",
    "glucose_non_diabetic = df_diabetes[df_diabetes[\"y\"] == 0][\"Glucose\"]\n",
    "# Define the bins (ranges) for the histogram\n",
    "bins = [50, 70, 100, 125, 150, 200, 250]\n",
    "\n",
    "# Plot histogram for diabetic patients\n",
    "plt.hist(glucose_diabetic, bins=bins, alpha=0.5, label=\"Diabetic\", color=\"red\")\n",
    "\n",
    "# Plot histogram for non-diabetic patients\n",
    "plt.hist(glucose_non_diabetic, bins=bins, alpha=0.5, label=\"Non-Diabetic\", color=\"blue\")\n",
    "\n",
    "# Labeling and legend\n",
    "plt.xlabel(\"Glucose Level\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Glucose Distribution by Diabetes Status\")\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value percentage checking\n",
    "missing_percentage = (df_diabetes.isnull().sum() / len(df_diabetes)) * 100\n",
    "\n",
    "# Show the missing percentage\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### Drop zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the 'y' column from the selection\n",
    "numeric_columns = df_diabetes.select_dtypes(include=[\"number\"]).drop(columns=[\"y\"])\n",
    "\n",
    "# Count the number of zeros in each selected column\n",
    "zero_counts = numeric_columns.eq(0).sum()\n",
    "\n",
    "# Print the columns with zero counts\n",
    "print(\"Columns with zero values and their counts (excluding 'y'):\")\n",
    "print(zero_counts[zero_counts > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Since `SkinThickness` and `Insulin` has many zeros, we drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"SkinThickness\", \"Insulin\", \"AgeRange\", \"BMIRange\"]\n",
    "df_new = df_diabetes.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "#### Treating the zero values in the rest of the columns\n",
    "\n",
    "We will replace the 0 values with mean / median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the skewness for all numerical columns\n",
    "skewness = df_new.skew()\n",
    "\n",
    "# Print the skewness values\n",
    "print(skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and median (excluding zeros)\n",
    "mean_glucose = df_new[df_new[\"Glucose\"] != 0][\"Glucose\"].mean()\n",
    "median_bmi = df_new[df_new[\"BMI\"] != 0][\"BMI\"].median()\n",
    "median_bp = df_new[df_new[\"BloodPressure\"] != 0][\"BloodPressure\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros across multiple columns at once\n",
    "df_new.replace(\n",
    "    {\n",
    "        \"Glucose\": {0: mean_glucose},\n",
    "        \"BMI\": {0: median_bmi},\n",
    "        \"BloodPressure\": {0: median_bp},\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the 'y' column from the selection\n",
    "numeric_columns = df_new.select_dtypes(include=[\"number\"]).drop(columns=[\"y\"])\n",
    "\n",
    "# Count the number of zeros in each selected column\n",
    "zero_counts = numeric_columns.eq(0).sum()\n",
    "\n",
    "# Print the columns with zero counts\n",
    "print(\"Columns with zero values and their counts (excluding 'y'):\")\n",
    "print(zero_counts[zero_counts > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Train - Test Data Split and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_new.values[:, :6]\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_new.values[:, 6:]\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your dataset into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{X_train.shape = }, {y_train.shape = }\")\n",
    "print(f\"{X_test.shape = }, {y_test.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "#### Apply SMOTE to the training data\n",
    "\n",
    "SMOTE is a statistical technique for dealing with imbalanced datasets, since we have an imbalance between diabetic and non-diabetic cases (~35% vs ~65% as shown in the pie chart). What SMOTE does:\n",
    "- Identifies minority class samples: In this case, patients with diabetes (class 1)\n",
    "- Creates synthetic examples: Instead of simply duplicating minority samples, SMOTE:\n",
    "    - Selects a minority class sample\n",
    "    - Finds its k-nearest neighbors (other minority samples)\n",
    "    - Generates new synthetic samples along the lines connecting the sample and its neighbors\n",
    "    - Balances the dataset: The resulting training data (`X_train_resampled`, `y_train_resampled`) has equal representation of both classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"{X_train_resampled.shape = }, {y_train_resampled.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_ones = np.sum(y_train_resampled == 1)\n",
    "num_zeros = np.sum(y_train_resampled == 0)\n",
    "\n",
    "print(f\"Number of 1s: {num_ones}\")\n",
    "print(f\"Number of 0s: {num_zeros}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "#### Scale the data\n",
    "Transforms each feature (column) using: `(x - mean) / std_dev`  \n",
    "Benefits:\n",
    "- Equalizes feature scales: Features like Glucose (70-200) and Pregnancies (0-17) are brought to the same scale\n",
    "- Improves neural network performance: The model won't be biased toward features with larger values\n",
    "- Speeds up gradient descent: Optimization converges faster with standardized data\n",
    "- Reduces sensitivity to outliers: Using unit variance makes the model more robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{X_train_resampled.shape = }, {y_train_resampled.shape = }\")\n",
    "print(f\"{X_test.shape = }, {y_test.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from loguru import logger\n",
    "from safetensors.torch import load_file, save_file\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.backends.xla.is_available():\n",
    "        return torch.device(\"xla\")\n",
    "    elif torch.backends.xpu.is_available():\n",
    "        return torch.device(\"xpu\")\n",
    "    elif torch.backends.ipu.is_available():\n",
    "        return torch.device(\"ipu\")\n",
    "    elif torch.backends.meta.is_available():\n",
    "        return torch.device(\"meta\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "DEVICE = get_device()\n",
    "logger.info(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_resampled)\n",
    "y_train_tensor = torch.FloatTensor(y_train_resampled).reshape(-1, 1)  # Add this reshape\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test).reshape(-1, 1)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset, batch_size=len(test_dataset), shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim=6):\n",
    "        super(Net, self).__init__()\n",
    "        # First layer with more units and batch normalization\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),  # Increased from 20 to 32\n",
    "            nn.BatchNorm1d(32),  # Added batch normalization\n",
    "            nn.LeakyReLU(0.1),  # LeakyReLU instead of ReLU\n",
    "            nn.Dropout(0.2),  # Increased dropout\n",
    "        )\n",
    "\n",
    "        # Second layer with more units\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(32, 24),  # Increased from 14 to 24\n",
    "            nn.BatchNorm1d(24),  # Added batch normalization\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "\n",
    "        # Third layer\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(24, 16), nn.BatchNorm1d(16), nn.LeakyReLU(0.1)\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Sequential(nn.Linear(16, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the improved model with weight decay\n",
    "model = Net().to(DEVICE)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Improved optimizer settings\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,  # Increased learning rate from 0.0001\n",
    "    weight_decay=0.0005,  # Added L2 regularization\n",
    ")\n",
    "\n",
    "# Add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"max\",\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "#### Train and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"./weights\").mkdir(parents=True, exist_ok=True)\n",
    "WEIGHT_PATH = Path(\"./weights/local_training_diabetes_model.safetensors\")\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=100):\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        history[\"train_loss\"].append(epoch_loss)\n",
    "        history[\"train_acc\"].append(epoch_acc)\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss, val_acc = evaluate_model(model, test_loader, criterion)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs} | \"\n",
    "            f\"Train Loss: {epoch_loss:.4f} | \"\n",
    "            f\"Train Acc: {epoch_acc:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} | \"\n",
    "            f\"Val Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            save_file(model.state_dict(), WEIGHT_PATH)\n",
    "            print(\n",
    "                f\"\\n Find best model with val_acc {best_val_acc}. saved model to {WEIGHT_PATH}\"\n",
    "            )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = train_model(model, train_loader, test_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model instance\n",
    "loaded_model = Net().to(DEVICE)\n",
    "\n",
    "# Load the weights from SafeTensors file\n",
    "loaded_model.load_state_dict(load_file(WEIGHT_PATH))\n",
    "\n",
    "# Evaluate the loaded model\n",
    "test_loss, test_acc = evaluate_model(loaded_model, test_loader, criterion)\n",
    "print(f\"Loaded model - Test loss: {test_loss:.4f}, Test accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
